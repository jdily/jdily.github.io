<!DOCTYPE html>
<!--[if lt IE 8 ]><html class="no-js ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="no-js ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 8)|!(IE)]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>

    <!--- Basic Page Needs
   ================================================== -->
    <meta charset="utf-8">
    <title>I-Chao Shen</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Mobile Specific Metas
   ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="css/default.css">
    <link rel="stylesheet" href="css/layout.css">
    <link rel="stylesheet" href="css/media-queries.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <!-- <link rel="stylesheet" href="css/academicons.min.css"> -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">


    <!-- Script
   ================================================== -->
    <script src="js/modernizr.js"></script>

    <!-- Favicons
	================================================== -->
    <link rel="shortcut icon" type="image/png" href="./images/ichao_potrait.png">

</head>

<body>

    <!-- Header
   ================================================== -->

    <!-- Style Guide Section
   ================================================== -->
    <section id="styles" style="background: #fff;">
        <br>

    <div id='top' class="row about">	
        <div class="two columns">
            <img src="./resource/ichao_new_photo.jpg" width="90%" alt="" align="middle" style="border-radius: 8%; padding-top: 5px"/>
        </div>
        <!-- <div class="two columns">
            
        </div> -->
        <div class="eight columns">
            <!-- <a style="float: right" href="./index_j.html">[Japanese]</a> -->
            <h4>I-Chao Shen</h4>
            <h6>ichaoshen@g.ecc.u-tokyo.ac.jp</h6> 
            <p>
                I am an assistant professor at Department of Computer Science, the Graduate School of Information Science and Technology, The University of Tokyo. 
                <!-- I am a member of the <a href="http://www-ui.is.s.u-tokyo.ac.jp/en/">User Interface Research Group</a> and working with <a href="https://www-ui.is.s.u-tokyo.ac.jp/~takeo/">Takeo Igarashi</a>. -->
                My research focus on computer graphics, with a focus on inverse modeling of 2D graphics and 2D/3D content generation.
                <!-- where I explore combinations between different human priors and numerical optimization  -->
                <!-- and develop supportive system for visual content generation. -->
                <br>
                <i class="ai ai-cv-square ai-1x"></i> <a href="./resource/ichao_cv.pdf"><b>C.V.</b></a>  &nbsp; | &nbsp; 
                    <i class="ai ai-google-scholar-square ai-1x"></i> 
                    <a href="https://scholar.google.com/citations?user=0XSoTYMAAAAJ&hl=en"><b>Google Scholar</b></a> &nbsp; | &nbsp;
                <i class="fa fa-envelope fa-1x"></i> <a href="mailto: jdilyshen@gmail.com"><b>Email</b></a> &nbsp; 
                <!-- | &nbsp; -->
                <!-- <i class="fa fa-calendar"></i> <a href="https://jdily.github.io/vchci-deadlines/?sub=CG,HCI,ML,CV,MM" target="_blank"><b>VC/HCI Calendar</b></a> -->
            </p>
        </div>
        <!-- <div class="vertical-line"></div> -->
        <div class="two columns">
            <p style="text-align: right;"><a href="#publication">Publication</a></p>
            <p style="text-align: right;"><a href="#preprint">Preprint</a></p>
            <p style="text-align: right;"><a href="#short">Short papers</a></p>
            <p style="text-align: right;"><a href="#fund">Research funds</a></p>
            <p style="text-align: right;"><a href="#service">Academic services</a></p>
            <p style="text-align: right;"><a href="#domestic">Domestic papers</a></p>
            <p style="text-align: right;"><a href="#misc">Miscellaneous</a></p>  
        </div>
    </div>

    <div class="row">
        <div id="publication" class="twelve columns">
            <p style="font-size:16px; padding-top: 5px; font-weight: bold;">Publication</p>
        </div>
    </div>
    <div class="row publication">
        <div class="twelve columns">
        <table border="0" cellpadding="3" cellspacing="10">
	        <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/nnproc/nnproc_eg25.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Approximating Procedural Models of 3D Shapes with Neural Networks</b><br>
                    Ishtiaque Hossain, <u>I-Chao Shen</u>, Oliver van Kaick <br>
                    Eurographics 2025 <br>
		            [<a href="./resource/nnproc/nnproc_paper.pdf">paper</a>]
                    [<a href="./resource/nnproc/nnproc_supp.pdf">supplemental material</a>]
                </td> 
            </tr>
	    
	        <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/fontcraft/fontcraft_img.jpg" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>FontCraft: Multimodal Font Design Using Interactive Bayesian Optimization</b><br>
                    Yuki Tatsukawa, <u>I-Chao Shen</u>, Mustafa Doga Dogan, Anran Qi, Yuki Koyama, Ariel Shamir, Takeo Igarashi <br>
                    CHI 2025 <br>
                    <font style="color:#787878">a system allows non-expert users to create new fonts using multi-modal input</font><br>
                    [<a href="./resource/fontcraft/fontcraft_paper.pdf">paper</a>]
                    [<a href="https://yukistavailable.github.io/fontcraft.github.io/">project page</a>]
                    [<a href="https://arxiv.org/abs/2502.11399">arxiv</a>]
                    [<a href="https://www.youtube.com/watch?v=KKSJQkWfS5o">video</a>]
                </td> 
            </tr>
	        <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/compact/compact_img.jpg" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>CompAct: Designing Interconnected Compliant Mechanisms with Active Material Integration</b><br>
                    Humphrey Yang, <u>I-Chao Shen</u>, Nikolas Martelaro, Bo Zhu, Haoran Xie, Takeo Igarashi, Lining Yao <br>
                    CHI 2025 <br>
		            [<a href="./resource/compact/compact_paper.pdf">paper</a>]
                    [<a href="./resource/compact/compact_video.mp4">video</a>]
                </td> 
            </tr>

            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/fontclip/fontclip_img.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications</b><br>
                    Yuki Tatsukawa, <u>I-Chao Shen</u>, Anran Qi, Yuki Koyama, Takeo Igarashi, Ariel Shamir <br>
                    Eurographics 2024 <br>
                    <font style="color:#787878">a visual language model that understand typography and support cross-lingual font retrieval and optimization</font><br>
                    [<a href="./resource/fontclip/fontCLIP_paper.pdf">paper</a>]
                    [<a href="https://yukistavailable.github.io/fontclip.github.io/">project page</a>]
		    [<a href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.15043">DOI</a>]
		    [<a href="./resource/fontclip/fontCLIP_supp.pdf">supplemental material</a>]
		    [<a href="https://github.com/yukistavailable/FontCLIP">code</a>]
            [<a href="https://arxiv.org/abs/2403.06453">arxiv</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/mokume/mokume_img.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Learned Inference of Annual Ring Pattern of Solid Wood</b><br>
                    <!-- <b>FontCLIP: A Semantic Typography Visual-Language Model for Multilingual Font Applications</b><br> -->
                    Maria Larsson<sup>*</sup>, Takashi Ijiri<sup>*</sup>, <u>I-Chao Shen</u>, Hironori Yoshida, Ariel Shamir, Takeo Igarashi (*: joint first authors) <br>
                    Computer Graphics Forum 2024 <br>
                    <!-- <font style="color:#787878">a visual language model that understand typography and support cross-lingual font retrieval and optimization</font><br> -->
		    [<a href='./resource/mokume/mokume_cgf.pdf'>paper</a>]
		    [<a href='https://onlinelibrary.wiley.com/doi/10.1111/cgf.15074'>DOI</a>]
                </td> 
            </tr>
            
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/vm_vton/vm_vton.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Virtual Measurement Garment for Per-Garment Virtual Try-On</b><br>
                    Zaiqiang Wu<sup>*</sup>, Jingyuan Liu<sup>*</sup>, Long Hin Toby Chong, <u>I-Chao Shen</u>, Takeo Igarashi (*: joint first authors)<br>
                    Graphics Interface 2024 <br>
                    <font style="color:#787878">a novel per-garment virtual try-on method which eliminates the need for the physical measurement garment</font><br>
                    [<a href="./resource/vm_vton/vm_vton_gi2024_cr.pdf">paper</a>]
                    [<a href="./proj_site/vm_vton_proj.html">project page</a>]
		            [<a href="https://youtu.be/nzimjrvU6h0">video</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/cache_vc/cache_thumb.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Improving Cache Placement for Efficient Cache-based Rendering</b><br>
                    Yu-Ting Wu, <u>I-Chao Shen</u><br>
                    The Visual Computer 2024 <br>
                    <font style="color:#787878">a new method for generating better cache distribution for cache-based rendering algorithms</font><br>             
		            [<a href="https://link.springer.com/article/10.1007/s00371-023-03231-z?fbclid=IwAR0rlvsAV69RnnAzlTzOvb-D3Tu2sPRzOC4V1q1ZBnqjtMi_kLDV917jS_o">paper</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/stylepart/stylepart_teaser.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>StylePart: Image-based Shape Part Manipulation</b><br>
                    <u>I-Chao Shen</u>, Li-Wen Su, Yu-Ting Wu, Bing-Yu Chen<br>
                    The Visual Computer 2024<br>
                    <font style="color:#787878">a latent mapping function that connects the image generative latent space and the 3D man-made shape attribute latent space</font><br>
                    [<a href="./resource/stylepart/stylepart_vc.pdf">paper</a>]
                    [<a href="./proj_site/stylepart_proj.html">project page</a>]
		            [<a href="./resource/stylepart/stylepart_supp.pdf">supplemental material</a>]
                    [<a href="https://arxiv.org/abs/2111.10520">arxiv</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/nerfin/nerfin_img.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>NeRF-In: Free-Form Inpainting for Pretrained NeRF with RGB-D Priors</b><br>
                    <u>I-Chao Shen</u><sup>*</sup>, Hao-Kang Liu<sup>*</sup>, Bing-Yu Chen (*: joint first authors) <br>
                    IEEE Computer Graphics and Applications (CG&A) 2023 <br>
                    <font style="color:#787878">an optimization framework that enables users to remove unwanted objects in a pre-trained NeRF</font><br>
                    [<a href="https://arxiv.org/abs/2206.04901">paper</a>]
                    [<a href="./proj_site/nerfin_proj.html">project page</a>]
                    [<a href="https://youtu.be/Wke_h28D2Tg">video</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/icon_colorization/vicon_colorization_image.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Palette-Based and Harmony-Guided Colorization for Vector Icons</b><br>
                    Miao Lin<sup>*</sup>, <u>I-Chao Shen</u><sup>*</sup>, Hsiao-Yuan Chin, Ruo-Xi Chen, Bing-Yu Chen (*: joint first authors)<br>
                    Computer Graphics Forum, Vol. 42, No. 7, (Proceedings of Pacific Graphics 2023 (PG 2023), Daejeon, Korea), 2023. <br>
                    <font style="color:#787878"> a palette-based colorization algorithm for vector icons without the need for rasterization. </font><br>
                    [<a href="./resource/icon_colorization/PG23_icon_colorization.pdf">paper</a>]
                    [<a href="./proj_site/icon_colorization.html">project page</a>]
		    [<a href="https://youtu.be/7aehJfJZxqA">video</a>]
                </td>
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/data_proc_shape/proc_shape_img.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Data-guided Authoring of Procedural Models of Shapes</b><br>
                    Ishtiaque Hossain, <u>I-Chao Shen</u>, Takeo Igarashi, Oliver van Kaick<br>
                    Computer Graphics Forum, Vol. 42, No. 7, (Proceedings of Pacific Graphics 2023 (PG 2023), Daejeon, Korea), 2023. <br>
                    <font style="color:#787878"> a data-guided assistive system that can create a procedural model to replicate a collection of reference shapes. </font><br>
                    [<a href="./resource/data_proc_shape/ProcShape_PG2023.pdf">paper</a>]
                    [<a href="./proj_site/data_proc_model.html">project page</a>]
			    [<a href="./resource/data_proc_shape/ProcShape_PG2023_SuppMat.pdf">supplemental material</a>]
		            [<a href="https://github.com/ishtiaquehossain-sys/SimpleProcShapes">code</a>]
                </td>
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/evicon/evicon_img.jpg" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>EvIcon: Designing High-Usability Icon with Human-in-the-loop Exploration and IconCLIP</b><br>
                    <u>I-Chao Shen</u>, Fu-Yin Cherng, Takeo Igarashi, Wen-Chieh Lin, Bing-Yu Chen <br>
                    Computer Graphics Forum, Volume 42, Issue 6, September 2023 <br>
                    <font style="color:#787878">an assistive system for designing high usability icons with a novel icon-specific visual-language model.</font><br>
                    [<a href="./resource/evicon/EvIcon_cgf_final.pdf">paper</a>]
                    [<a href="./proj_site/evicon_proj.html">project page</a>]
		    [<a href="https://youtu.be/uQucAoHd9Gg">video</a>]
                    [<a href="https://arxiv.org/abs/2305.17609">arxiv</a>]
		    [<a href="https://github.com/jdily/IconCEPT10K">IconCEPT10K dataset</a>]
                </td>
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/360mvsnet/360mvsnet_thumb.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>360MVSNet: Deep Multi-view Stereo Network with 360&#176 Images for Indoor Scene Reconstruction</b><br>
                    Ching-Ya Chiu, Yu-Ting Wu, <u>I-Chao Shen</u>, Yung-Yu Chuang <br>
                    IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023 (Algorithm Track)<br>
                    <font style="color:#787878">a deep learning network for multi-view stereo with 360&#176 images.</font><br>
                    [<a href="./resource/360mvsnet/360mvsnet_main.pdf">paper</a>]
                    [<a href="./proj_site/360mvsnet_proj.html">project page</a>]
                    [<a href="./resource/360mvsnet/360mvsnet_supp.pdf">supplemental material</a>]
                </td>
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="https://kevincosner.github.io/publications/Chung2022SFU/thumb.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>StyleFaceUV: a 3D Face UV Map Generator for View-Consistent Face Image Synthesis</b><br>
                    Wei-Chieh Chung<sup>*</sup>, Jian-Kai Zhu<sup>*</sup>, <u>I-Chao Shen</u>, Yu-Ting Wu, Yung-Yu Chuang (*: joint first authors)<br>
                    British Machine Vision Conference (BMVC) 2022 <br>
                    <font style="color:#787878">an approach for generating detailed 3D faces using a pre-trained StyleGAN2 model.</font><br>
                    [<a href="./resource/StyleFaceUV/StyleFaceUV_main.pdf">paper</a>]
                    [<a href="./proj_site/stylefaceuv_proj.html">project page</a>]
                    [<a href="./resource/StyleFaceUV/StyleFaceUV_supp.pdf">supplemental material</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/oden/ODEN_fig.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>ODEN: Live Programming for Neural Network Architecture Editing</b><br>
                    Chunqi Zhao, <u>I-Chao Shen</u>, Tsukasa Fukusato, Jun Kato, Takeo Igarashi<br>
                    In Proceedings of 26th International Conference on Intelligent User Interfaces (IUI 2022), Online, 2022.03.22-25.<br>
                    <font style="color:#787878">a intelligent neural network architecture editing system using live programming techniques</font><br>
                    [<a href="./resource/iui2022/iui22-18.pdf">paper</a>]
                    [<a href="https://haremi.xyz/projects/oden/oden.html">project page</a>]
                    [<a href="https://youtu.be/i4OGVdfgFxU">demo video</a>]
                    [presentation video]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/mannequin_teaser.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Per Garment Capture and Synthesis for Real-time Virtual Try-on</b><br>
                    Toby Chong, <u>I-Chao Shen</u>, Nobuyuki Umetani, Takeo Igarashi  <br>
                    in proceeding of User Interface Software and Technology (UIST) 2021  <br>
                    <font style="color:#787878">a real-time virtual try-on using per garment capture and synthesis workflow</font><br>
                    [<a href="https://drive.google.com/file/d/1TtjZYr_bMtwu5YRlEPoI34aCuryyRkkj/view?usp=sharing">paper</a>]
                    [<a href="https://sites.google.com/view/deepmannequin">project page</a>]
                    [<a href="https://drive.google.com/file/d/1oBQhjAecSXSRLCyYKu1BocANtuRit1SC/view?usp=sharing">video</a>]
                    [<a href="https://arxiv.org/abs/2109.04654">arxiv</a>]
                    [<a href="https://www.u-tokyo.ac.jp/focus/en/press/z0508_00191.html">UTokyo english press</a>]
                    [<a href="https://sj.jst.go.jp/news/202202/n0209-01j.html">Science Japan (JST)</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/ddsb_thumbnail.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Data-driven Sketch Beautification with Neural Feature Representation</b><br>
                    <u>I-Chao Shen</u> <br>
                    IEEE Computer Graphics and Applications (CG&A) 2021 <br>
                    <font style="color:#787878">a data-driven approach for beautifying freehand sketches</font><br>
                    [<a href="../resource/ddsb/ddsb_cga.pdf">paper</a>]
                    [<a href="./proj_site/ddsb_proj.html">project page</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/hpg21_thumb.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Multi-Resolution Shared Representative Filtering for Real-Time Depth Completion</b><br>
                    Yu-Ting Wu, Tzu-Mao Li, <u>I-Chao Shen</u>, Hong-Shiang Lin, Yung-Yu Chuang  <br>
                    High-Performance Graphics (HPG) 2021  <br>
                    <font style="color:#787878">a real-time depth completion algorithm which can effectively handle large missing regions of depth maps</font><br>
                    [<a href="https://kevincosner.github.io/publications/Wu2021MSR/paper.pdf">paper</a>]
                    [<a href="https://kevincosner.github.io/publications/Wu2021MSR/index.html">project page</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/clipgen_img.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>ClipGen: A Deep Generative Model for Clipart Vectorization and Synthesis</b><br>
                    <u>I-Chao Shen</u>, Bing-Yu Chen  <br>
                       Transaction on Visualization and Computer Graphics (TVCG), 2021 <br>
                    <font style="color:#787878">a novel deep learning-based approach for automatically vectorizing and synthesizing the clipart of man-made objects</font><br>
                    [<a href="../resource/clipgen/ClipGen_tvcg_final.pdf">paper</a>]
                    [<a href="./proj_site/clipgen_proj.html">project page</a>]
                    [<a href="http://arxiv.org/abs/2106.04912">arxiv</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/clipflip.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b> ClipFlip : Multi-view Clipart Design</b><br>
                    <u>I-Chao Shen</u>, Kuan-Hung Liu, Li-Wen Su, Yu-Ting Wu, Bing-Yu Chen<br>
                    Computer Graphics Forum, Volume 40, Issue 1, Feb 2021 <br>
                    <font style="color:#787878">an assistive system for clipart design by providing visual scaffolds from the unseen viewpoints</font><br>
                    [<a href="../resource/clipflip/clipflip_final_paper.pdf">paper</a>]
                    [<a href="./proj_site/clipflip_proj.html">project page</a>]
                    [<a href="../resource/clipflip/clipflip_supplemental_final.pdf">supplemental material</a>]
                    <!-- [<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14190">publisher version</a>]    -->
                    [<a href="http://arxiv.org/abs/2008.12933">arxiv</a>]
                </td> 
            </tr>           
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/ganui_img.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Interactive Optimization of Generative Image Modeling using Sequential Subspace Search and Content-based Guidance </b><br>
                    Toby Chong Long Hin<sup>*</sup>, <u>I-Chao Shen</u><sup>*</sup>, Issei Sato, Takeo Igarashi (*: joint first authors) <br>
                    Computer Graphics Forum, Volume 40, Issue 1, Feb 2021 <br>
                    <font style="color:#787878">a human-in-the-optimization method that allows users to directly explore and search the latent vector space of generative image modeling</font><br>
                    [<a href="../resource/ganui/GANUI_cgf_final.pdf">paper</a>]      
                    [<a href="./proj_site/ganui_proj.html">project page</a>]
                    <!-- [<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14188">publisher version</a>]  -->
                    [<a href="http://arxiv.org/abs/1906.09840">arxiv</a>]
                </td> 
            </tr>       
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/MOAI_real.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b> ZomeFab: Cost-effective Hybrid Fabrication with Interior Zometool Structure</b><br>
                    <u>I-Chao Shen</u>, Ming-Shiuan Chen, Chun-Kai Huang, Bing-Yu Chen  <br>
                    Computer Graphics Forum, Volume 39, Issue 1, Feb 2020 <br>
                    <font style="color:#787878">a hybrid 3D fabrication method that combines 3D printing and the Zometool construction set to achieve compact fabrication</font><br>
                    [<a href="../resource/zomeFab_cgf2019/zomeFab_cgf_final.pdf">paper</a>]
                    [<a href="./proj_site/zomeFab_proj.html">project page</a>]
                    <!-- [<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.13805">publisher version</a>]  -->
                    [<a href="http://arxiv.org/abs/1906.09787">arxiv</a>]
                </td> 
            </tr>     
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/transfer_rl_rep_img.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Transferring Deep Reinforcement Learning with Adversarial Objective and Augmentation</b><br>
                    <u>I-Chao Shen</u>, Shu-Hsuan Hsu, Bing-Yu Chen  <br>
                    to appear in IJCAI-PRICAI 2020 Workshop on Knowledge Based Reinforcement Learning (KBRL) <br>
                    <font style="color:#787878">a semi-supervised transfer learning method for deep reinforcement learning</font><br>
                    [<a href="../resource/IJCAI_KBRL_RL_transfer.pdf">paper</a>]
                    [<a href="http://arxiv.org/abs/1809.00770">arXiv</a>]
                    [<a href="https://www.youtube.com/watch?v=zYo02n9c5YY">present video</a>]
                </td> 
            </tr>   
            <tr>
                <td style="padding-right: 10px">
                    <img src="https://people.cs.nycu.edu.tw/~liweichan/placeholders/project_icons/director360_icon.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Director-360: Introducing Camera Handling to 360 Cameras</b><br>
                    Hao-Juan Huang, <u>I-Chao Shen</u>, Liwei Chan  <br>
                    in proceeding of MobileHCI 2020 <br>
                    <!-- <font style="color:#D3D3D3">a semi-supervised transfer learning method for deep reinforcement learning</font><br> -->
                    [<a href="https://dl.acm.org/doi/10.1145/3379503.3403550">paper</a>]
                </td> 
            </tr>   
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/sig18_vec.jpg" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Perception-Driven Semi-Structured Boundary Vectorization</b><br>
                    Shayan Hoshyari, Edoardo A. Dominici, Alla Sheffer, Nathan Carr, Duygu Ceylan,  <br> Zhaowen Wang, <u>I-Chao Shen</u> <br>
                    ACM Transaction on Graphics (Proceedings of SIGGRAPH 2018)<br> 
                    <font style="color:#787878">a boundary vectorization approach leverages human perception of shapes to
                        generate vector images consistent with viewer expectations</font><br>
                        [<a href="https://drive.google.com/open?id=1X_vwr2yhpMqxd8TTXHwQqkte_TpNKrbD">paper</a>]
                        [<a href="http://www.cs.ubc.ca/labs/imager/tr/2018/PerceptionDrivenVectorization/">project page</a>]
                    [<a href="./bibs.html#sig18_vec">bib</a>]
                </td> 
            </tr>  
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/17_PG_360video.jpg" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>High-resolution 360 Video Foveated Stitching for Real-time VR</b><br>
                    Wei-Tse Lee*, Hsin-I Chen*, Ming-Shiuan Chen, <u>I-Chao Shen</u>, Bing-Yu Chen (*: joint first authors) <br>
                    Computer Graphics Forum (Proceedings of Pacific Graphics 2017) <br> 
                    <font style="color:#787878">a real-time 360◦ video foveatedstitching framework</font><br>
                    [<a href="https://drive.google.com/open?id=1S7E5O52dTOK_JRAg_EnqzQjwWNTIJEWN">paper</a>]
                    [<a href="./resource/PG17_360/pg_present.pdf">slide</a>]
                    [<a href="./bibs.html#pg16_retarget">bib</a>]
                </td> 
            </tr>  
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/16_SIGGRAPHAsia_ShapeNetSeg.jpg" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b> A Scalable Active Framework for Region Annotation in 3D Shape Collections</b><br>
                    Li Yi, Vladimir G. Kim, Duygu Ceylan, <u>I-Chao Shen</u>, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas Guibas <br> ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2016) <br> 
                    <font style="color:#787878">a novel active learning method capable of enriching massive geometric datasets with accurate semantic region annotations</font><br>
                    [<a href="https://drive.google.com/open?id=1cYR37vrKHXSkEVBKC7eEyr1MVRndZqNx">paper</a>]
                    [<a href="http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html">project page</a>] 
                    [<a href="http://web.stanford.edu/~ericyi/papers/part_annotation_16_supplemental.pdf">supplemental</a>]
                    [<a href="./bibs.html#siga16_active/">bib</a>]
                </td> 
            </tr>    
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/16-PG-Retarget.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Retargeting 3D Objects and Scenes with a General Framework</b><br>
                    Chun-Kai Huang, Yi-Ling Chen, <u>I-Chao Shen</u>, Bing-Yu Chen <br> Computer Graphics Forum (Proceedings of Pacific Graphics 2016) <br>
                    <font style="color:#787878">an interactive method suitable for retargeting both 3D objects and scenes</font><br>
                    [<a href="https://drive.google.com/open?id=1TLfE4O65oYSmp399Pzwgg03kOKSfnnbU">paper</a>]
                    [<a href="http://www.cmlab.csie.ntu.edu.tw/~chinkyell/project/regtargeting.html">project page</a>]
                    [<a href="https://www.dropbox.com/s/q2i63fr5yrkkbfc/PG2016-SceneEditing-SuppMat-fin.pdf?dl=0">supplemental</a>]
                    [<a href="https://www.dropbox.com/s/vzaxe5hlsjra2wr/pg16-sceneRetargeting_slide.pptx?dl=0">slide</a>]
                    [<a href="./bibs.html#pg16_retarget">bib</a>]
                </td> 
            </tr>   
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/PG2015-hrs.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Data-driven Handwriting Synthesis in a Conjoined Manner</b><br>
                    Hsin-Yi Chen, Tse-Ju Lin, <u>I-Chao Shen</u>, Bing-Yu Chen <br> Computer Graphics Forum (Proceedings of Pacific Graphics 2015) <br>
                    <font style="color:#787878">a novel method for synthesizing handwritten text according to the writer’s style while considering characters’ conjoined property </font><br>   
                    [<a href="https://drive.google.com/open?id=1EIh2FGLuRWbQQ5AhgHNnNmEs-GTyHja8">paper</a>]
                    [<a href="http://graphics.csie.ntu.edu.tw/~fensi/projects/handwriting/">project page</a>]
                    [<a href="http://graphics.csie.ntu.edu.tw/~fensi/projects/handwriting/PG2015-hrs.pptx">slide</a>]
                    [<a href="./bibs.html#pg15_handwriting">bib</a>]
                </td> 
            </tr>    
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/2015TMM-GF.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Gestalt Rule Feature Points</b><br>
                    <u>I-Chao Shen</u>, Wen-Huang Cheng <br> IEEE Transactions on Multimedia (TMM), volume 17, number 4, page 526-537, April 2015.<br>
                    <font style="color:#787878">a new approach for detecting reliable visual features from images in different visual styles (e.g., a photo and a painting) </font><br>
                    [<a href="https://drive.google.com/open?id=1OGuar_Old21Rw8Q3Bbnsrsc-0xWiQn99">paper</a>]
                    <!-- [<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=7047900&queryText%3Dgestalt+rule+feature+point">digital library</a>] -->
                    [<a href="./bibs.html#tmm15_gestalt">bib</a>]
                </td> 
            </tr>  
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/TVCG14-StereoSyn.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Geometrically Consistent Stereoscopic Image Editing using Patch-based Synthesis</b><br>
                    Sheng-Jie Luo, Ying-Tse Sun, <u>I-Chao Shen</u>, Bing-Yu Chen, Yung-Yu Chuang, <br> IEEE Transactions on Visualization and Computer Graphics (TVCG), Vol.21, No. 1, pp. 56-67, January 2015. <br>
                    <font style="color:#787878">a patch-based synthesis framework for stereoscopic image editing problems, including depth-guided texture synthesis, stereoscopic NPR, paint by depth, content adaptation, and 2D to 3D
                    conversion </font><br>
                    [<a href="https://drive.google.com/open?id=1VCTQkPdfqdUWPA-zKv2muY-beFusefCR">paper</a>]
                    [<a href="http://www.cmlab.csie.ntu.edu.tw/~forestking/research/TVCG14-StereoSyn/">project page</a>]
                    <!-- [<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6824802">Digital Library</a>] -->
                    [<a href="./bibs.html#tvcg13_stereo_patch">bib</a>]
                </td> 
            </tr> 
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/PG2013-Skeleton.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Stroke-guided Image Synthesis for Skeletal Structure Editing</b><br>
                    Sheng-Jie Luo, Chin-Yu Lin, <u>I-Chao Shen</u>, Bing-Yu Chen, <br> Computer Graphics Forum (Pacific Graphics 2013) <br>
                    <font style="color:#787878">a technique for synthesizing image objects with different skeletal structures while respecting to an input image object</font><br>
                    [<a href="https://drive.google.com/open?id=1qAESxb4ZE6XDf1JRgpEQo_qjzv5OrFjr">paper</a>]
                    [<a href="http://www.cmlab.csie.ntu.edu.tw/~forestking/research/PG13-SkeletalEditing/">project page</a>]
                    [<a href="http://www.cmlab.csie.ntu.edu.tw/~forestking/research/PG13-SkeletalEditing/resources/pg13_skeleton_fin.pptx">slide</a>]
                    <!-- [<a href="http://diglib.eg.org/EG/DL/CGF/volume32/issue7">Digital Library</a>] -->
                    [<a href="./bibs.html#pg13_stroke">bib</a>]
                </td> 
            </tr> 
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/SIGGRAPHASIA2012-stereocloning.png" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td>
                <td>
                    <b>Perspective-Aware Warping for Seamless Stereoscopic Image Cloning</b><br>
                    Sheng-Jie Luo, <u>I-Chao Shen</u>, Bing-Yu Chen, Wen-Huang Cheng, Yung-Yu Chuang, <br> SIGGRAPH ASIA 2012 <br>
                    <font style="color:#787878">a novel technique for seamless stereoscopic image cloning, which performs both shape adjustment and color blending</font><br>
                    [<a href="https://drive.google.com/open?id=1leH6uRjaBZa9oyelk0FVcE3UaTWGI72T">paper</a>]
                    [<a href="http://www.cmlab.csie.ntu.edu.tw/~forestking/research/SIGA12-StereoCloning/">project page</a>]
                    <!-- [<a href="http://doi.acm.org/10.1145/2366145.2366201">Digital Library</a>] -->
                    [<a href="./bibs.html#siga12_cloning">bib</a>]
                </td> 
            </tr>   
        </table>
        </div>
        <p style="text-align: right;"><a href="#top">[top]</a></p>
    </div>
    <hr>
    <div class="row">
        <div id="preprint" class="twelve columns">
            <p style="font-size:16px; padding-top: 5px; font-weight: bold;">Preprint</p>
        </div>
    </div>
    <div class="row publication">
        <div class="twelve columns">
        <table border="0" cellpadding="3" cellspacing="10">
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/autosketch/autosketch.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>AutoSketch: VLM-assisted Style-Aware Vector Sketch Completion</b><br>
                    Hsiao-Yuan Chin<sup>*</sup>, <u>I-Chao Shen</u><sup>*</sup>, Yi-Ting Chiu, Bing-Yu Chen (*: joint first authors)<br>
                        arXiv, 2025 <br>
                    <!-- <font style="color:#787878">a user-assisted method to enhance the quality of 3DGS avatars.</font><br> -->
                    <!-- [<a href="./proj_site/autopoly_proj.html">project page</a>] -->
                    [<a href="https://arxiv.org/abs/2502.06860">arxiv</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/avatarperfect/avatarperfect.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>AvatarPerfect: User-Assisted 3D Gaussian Splatting Avatar Refinement with Automatic Pose Suggestion</b><br>
                    Jotaro Sakamiya, <u>I-Chao Shen</u>, Jinsong Zhang, Mustafa Doga Dogan, Takeo Igarashi<br>
                        arXiv, 2024 <br>
                    <font style="color:#787878">a user-assisted method to enhance the quality of 3DGS avatars.</font><br>
                    <!-- [<a href="./proj_site/autopoly_proj.html">project page</a>] -->
                    [<a href="https://arxiv.org/abs/2412.15609">arxiv</a>]
                </td> 
            </tr>
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/autopoly/autopoly_img.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>AutoPoly: Predicting a Polygonal Mesh Construction Sequence from a Silhouette Image</b><br>
                    <u>I-Chao Shen</u>, Yu Ju (Edwin) Chen, Oliver van Kaick, Takeo Igarashi<br>
                        arXiv, 2022 <br>
                    <font style="color:#787878">a hybrid method that generates a polygonal mesh construction sequence from a silhouette image</font><br>
                    [<a href="https://arxiv.org/abs/2203.15233">arxiv</a>]
                    [<a href="./proj_site/autopoly_proj.html">project page</a>]
                </td> 
            </tr>
        </table>
        <p style="text-align: right;"><a href="#top">[top]</a></p>
        </div>
    </div>
    <hr> 
    <div class="row">
        <div id="short" class="twelve columns">
            <p style="font-size:16px; padding-top: 5px; font-weight: bold;">Short Paper, Poster, and Demo</p>
        </div>
    </div>
    <div class="row publication">
        <div class="twelve columns">
        <table border="0" cellpadding="3" cellspacing="10">
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/reconfig_joint/reconfig_joint.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>Designing Reconfigurable Joints</b><br>
	            Atsushi Maruyama, Maria Larsson, <u>I-Chao Shen</u>, Takeo Igarashi<br>
                    SIGGRAPH ASIA 2024 Technical Communication <b>(Honorable Mention Award)</b><br>
                    [<a href='./resource/reconfig_joint/reconfig_joint_tcomm.pdf'>paper</a>]
                </td> 
            </tr> 
	    <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/dual_avatar/dual_avatar.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>DualAvatar: Robust Gaussian Splatting Avatar with Dual Representation</b><br>
		    Jinsong Zhang, <u>I-Chao Shen</u>, Jotaro Sakamiya, Yu-Kun Lai, Takeo Igarashi, Kun Li<br>
                    SIGGRAPH 2024 Poster <br>
                    [<a href='./resource/dual_avatar/dual_avatar_siga24_poster.pdf'>paper</a>]
	            [<a href='./resource/dual_avatar/dual_avatar_print_poster.pdf'>poster</a>]
                </td> 
            </tr> 
	    <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/var_font/var_font_siga_poster.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>Generating Font Variations Using Latent Space Trajectory</b><br>
		    Sotaro Kanazawa, <u>I-Chao Shen</u>, Yuki Tatsukawa, Takeo Igarashi<br>
                    SIGGRAPH 2024 Poster <br>
                    [<a href='./resource/var_font/var_font_siga24_poster.pdf'>paper</a>]
		    [<a href='./resource/var_font/var_font_print_poster.pdf'>poster</a>]
                </td> 
            </tr> 
	    <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/vton_rtl/vton_rtl_img.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>Real-Time Virtual Try-On Using Generative AI</b><br>
		    Zaiqiang Wu, <u>I-Chao Shen</u>, Yuki Shibata, Takayuki Hori, Mengjia Jin, Wataru Kubo, Takeo Igarashi<br>
                    SIGGRAPH 2024 Real-Time Live! <br>
                </td> 
            </tr> 
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/hidden_recon/hidden_recon_img.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>3D Reconstruction from Sketch with Hidden Lines by Two-Branch Diffusion Model</b><br>
	            Yuta Fukushima, Anran Qi, <u>I-Chao Shen</u>, Yulia Gryaditskaya, Takeo Igarashi<br>
                    Eurographics 2024 Short Paper <br>
                    [<a href='./resource/hidden_recon/hidden_recon_eg24_short.pdf'>paper</a>]
                </td> 
            </tr>  
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/microglam.jpeg" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>MicroGlam: Microscopic Skin Image Dataset with Cosmetics</b><br>
	            Toby Chong, Alina Chadwick, <u>I-Chao Shen</u>, Haoran Xie, Takeo Igarashi<br>
                    SIGGRAPH ASIA 2023 Technical Communication <br>
                    [<a href="./resource/microglam/MakeupSA2023_tech_comm_small.pdf">paper</a>]
                </td> 
            </tr>  
	    <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/nebuta/nebuta2.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>Computational Design of Nebuta-like Paper-on-Wire Artworks</b><br>
		    Naoki Agata, Anran Qi, Yuta Noma, <u>I-Chao Shen</u>, Takeo Igarashi<br>
                    SIGGRAPH 2023 Poster <br>
                    [<a href="./resource/nebuta/nebuta_siggraph23_poster.pdf">paper</a>]
	            [<a href="./resource/nebuta/nebuta_poster_design.pdf">poster</a>]
                </td> 
            </tr>  
	    <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/icon_colorization/vicon_colorization_image.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>Palette-Based Colorization for Vector Icons</b><br>
		    Miao Lin, <u>I-Chao Shen</u>, Hsiao-Yuan Chin, Ruo-Xi Chen, Bing-Yu Chen<br>
                    SIGGRAPH 2023 Poster <br>
                    [<a href="./resource/icon_colorization/vicon_colorization_siggraph23poster.pdf">paper</a>]
	            [<a href="./resource/icon_colorization/vicon_colorization_poster_design.pdf">poster</a>]
                </td> 
            </tr>  
            <tr>
                <td style="padding-right: 10px">
                    <img src="./resource/stencil_art/siga2022_stencil_art.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>OVERPAINT: Automatic Multi-Layer Stencil Generation without Bridges</b><br>
	            Yuta Fukushima, Anran Qi, <u>I-Chao Shen</u>, Takeo Igarashi<br>
                    SIGGRAPH ASIA 2022 Technical Communication <br>
                    [<a href="./resource/stencil_art/Stencil_Generation_siga2022_tcom.pdf">paper</a>]
                </td> 
            </tr>  
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/img2img_translate.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>Guided Image Weathering using Image-to-Image Translation</b><br>
                    Li-Yu Chen, <u>I-Chao Shen</u>, Bing-Yu Chen<br>
                   SIGGRAPH ASIA 2021 Technical Communication <br>
                   [<a href="./resource/sa21_weathering/SA21_TC_Weathering.pdf">paper</a>]
                   [<a href="https://youtu.be/EqRucsDFzOQ">presentation video</a>]
                </td> 
            </tr>   
            <tr>
                <td style="padding-right: 10px">
                <img src="./images/mannequin_teaser.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>Real-time Image-based Virtual Try-on with Measurement Garment</b><br>
                    Toby Chong, <u>I-Chao Shen</u>, Yunfei Qian, Nobuyuki Umetani, Takeo Igarashi<br>
                    SIGGRAPH ASIA 2021 Emerging Technologies <br>
                   [<a href="./resource/sa21_etech_mannequin/Mannequin_sigasia_etech2021.pdf">paper</a>]
                </td> 
            </tr>   
            <tr>
                <td style="padding-right: 10px">
                    <img src="./images/PG2012-Disparity.png" style="vertical-align: text-top;" width="100" height="inherit"/>
                </td>
                <td>
                    <b>User-Assisted Disparity Maps</b><br>
                    Hsin-Yi Chen, Yi-Shan Lin, <u>I-Chao Shen</u>, Sheng-Jie Luo, Wen-Huang Cheng, Bing-Yu Chen,<br> Pacific Graphics 2012 Short Paper <br>
                    <font style="color:#787878">an intuitive and efficient system for correcting the artifacts and noises caused by imperfect disparity estimation</font><br>
                    [<a href="http://www.cmlab.csie.ntu.edu.tw/project/udm/">project Page</a>]
                    [<a href="https://drive.google.com/open?id=1x1rJwraWb0pwEM0OwuIOsYez-XuplIHg">paper</a>]
                    [<a href="./bibs.html#pg12_disparity">bib</a>]
                </td> 
            </tr>     
        </table>
        </div>
        <p style="text-align: right;"><a href="#top">[top]</a></p>
    </div> 

    <hr> 
    <div class="row">
        <div id="fund" class="twelve columns">
            <p style="font-size:16px; padding-top: 5px; font-weight: bold;">Research Funds</p>
        </div>
    </div>
    <div class="row publication">
        <div class="twelve columns">
            <ll>
                <li style="margin-top: -0px">JSPS Kakenhi Grant-in-Aid for Young Scientists (科研費 若手研究), 2023 [<a href="https://kaken.nii.ac.jp/ja/grant/KAKENHI-PROJECT-23K16921/">Link</a>]</li>
                <li style="margin-top: -15px">JST AIP Challenge Researcher (CREST AIPチャレンジプログラム), 2021 [<a href="https://www.jst.go.jp/kisoken/aip/program/wakate/challenge/list2021.html">Link</a>]</li>
                <li style="margin-top: -15px">JSPS Grant-in-Aid for Scientific Research for JPSP foreign fellow (外国人特別研究員), 2021 [<a href="https://www.jsps.go.jp/file/storage/general/j-fellow/saiyo/pdf/r2/2020-1-j.pdf">Link</a>]</li>
            </ll>
        </div>
        <p style="text-align: right;"><a href="#top">[top]</a></p>
    </div>

    <hr>
    <div class="row">
        <div id="service" class="twelve columns">
            <p style="font-size:16px; padding-top: 5px; font-weight: bold;">Academic services</p>
        </div>
    </div>
    <div class="row publication">
        <div class="twelve columns">
            <ll>
                <!-- <li style="margin-top: -0px">Technical Papers Committee, SIGGRAPH Asia 2025 [Link]</li> -->
                <!-- <li style="margin-top: -0px">International Program Committee, Pacific Graphics 2025 [Link]</li> -->
                <li style="margin-top: -0px">Short Papers Committee, Eurographics 2025 [Link]</li>
                <li style="margin-top: -15px">Technical Papers COI Coordinator, SIGGRAPH 2025 [<a href="https://s2025.siggraph.org/technical-papers-committee/">Link</a>]</li>
                <li style="margin-top: -15px">Technical Papers Committee, SIGGRAPH Asia 2024 [<a href="https://asia.siggraph.org/2024/about-the-event/siggraph-asia-2024-committee/technical-papers-committee/">Link</a>]</li>
                <li style="margin-top: -15px">Technical Program Committee, AAAI 2023 [<a href="https://aaai.org/conference/aaai/aaai-23/aaai-23-program-committee/">Link</a>], 2024 [<a href="https://aaai.org/aaai-24-conference/aaai-24-program-committee/">Link</a>], 2025 [<a href="https://aaai.org/conference/aaai/aaai-25/aaai-25-program-committee/">Link</a>]</li>
                <li style="margin-top: -15px">Reviewer: SIGGRAPH, SIGGRAPH ASIA, Pacific Graphics, Eurographics, NeurIPS, ICLR, ICML, CVPR, CHI, UIST, WACV, BMVC</li>
            </ll>
        </div>
        <p style="text-align: right;"><a href="#top">[top]</a></p>
    </div>


    <hr> 
    <div class="row">
        <div id="domestic" class="twelve columns">
            <p style="font-size:16px; padding-top: 5px; font-weight: bold;">Domestic (Japan) Papers and Posters</p>
        </div>
    </div>
    <div class="row publication">
        <div class="twelve columns">
        <table border="0" cellpadding="3" cellspacing="10">
            <tr>
                <td>
                    <b>個別衣服の学習によるリアルタイム仮想試着</b>, Zaiqiang Wu, Jingyuan Liu, Yechen Li, Toby Chong, <u>I-Chao Shen</u>, 五十嵐 健夫 (東京大学), 金 孟佳, 久保 渉, 柴田 勇希, 堀 隆之, 田之上 隼人, 西原 大輝 (ソフトバンク) (デモ)
                </td>
            </tr>
            <tr>
                <td>
                    <b>Human-in-the-loopによる3D Gaussian Splattingのゴミ除去手法</b>, 坂宮 丞太郎, Jinsong Zhang, <u>I-Chao Shen</u>, Takeo Igarashi. VC 2024 Paper (ショート発表)
                </td> 
            </tr>  
            <tr>
                <td>
                    <b>自己教師あり学習を用いたモーションの連続性の学習, Naoki Agata, <u>I-Chao Shen</u>, Takeo Igarashi. VC 2024 Paper (ロング発表)
                </td> 
            </tr>  
            <tr>
                <td>
                    <b>「ねぶた風紙細工」をデザインするためのアルゴリズム</b>, Naoki Agata, Anran Qi, Yuta Noma, <u>I-Chao Shen</u>, Takeo Igarashi. VC 2023 Paper (ロング発表)
                </td> 
            </tr>  
            <tr>
                <td>
                    <b>タイポグラフィに特化した視覚言語モデル</b>, 裕貴 立川, 奕超 沈,   安然 祁, 裕己 小山,  健夫 五十嵐, and アリエル シャミール. VC 2023 Paper (ショート発表)
                </td> 
            </tr>  
            <tr>
                <td>
                    <b>OVERPAINT: 橋を用いない多層ステンシルの自動生成</b>, Yuta Fukushima, Anran Qi, <u>I-Chao Shen</u>, Takeo Igarashi. VC 2022 Poster
                </td> 
            </tr>  
        </table>
        </div>
        <p style="text-align: right;"><a href="#top">[top]</a></p>
    </div>

    <div class="row">
        <div id='misc' class="twelve columns">
            <p style="font-size:16px; padding-top: 5px; font-weight: bold;">Misc</p>
        </div>
    </div>
    <div class="row publication">
        <div class="twelve columns">
        <table border="0" cellpadding="3" cellspacing="10">
            <tr>
                <td>
                    <b>Visual Computing and HCI conference information</b> [<a href="https://jdily.github.io/vchci-deadlines/?sub=HCI,ML,CV,MM,CG">link</a>] <br>
                    <font style="color:#787878">if you have conference information you want to add to the list, welcome to send a pull request.</font>
                </td>
            </tr>
            <tr>
                <td>
                    <b>A Curated List of Computational Font Typography Research</b> [<a href="https://github.com/jdily/font-typography-curate-list">link</a>] <br>
                    <font style="color:#787878">a collection of researches of computational font and typography research.</font><br>
                </td> 
            </tr>  
            <tr>
                <td>
                    <b>Latex template post-process scripts</b> [<a href="https://github.com/jdily/latex_post_processor">link</a>] <br>
                    <font style="color:#787878">a collection of scripts to post-process the latex code to be ready to upload to arxiv or submit the camera-ready version.</font><br>
                </td> 
            </tr>  
            <tr>
                <!-- <td style="padding-right: 10px">
                    <img src="" style="vertical-align: text-top;" width="150" height="inherit"/>
                </td> -->
                <td>
                    <b>All-in-one latex template</b> [<a href="https://github.com/jdily/all_conf_template">link</a>] <br>
                    <font style="color:#787878">a latex paper template that supports various conferences across research fields including computer graphics, computer vision, and hci.</font><br>
                </td> 
            </tr>  
        </table>
        </div>
        <p style="text-align: right;"><a href="#top">[top]</a></p>
    </div>

        <!-- footer
   ================================================== -->
        <footer>
            <div class="row">
                <div class="twelve columns">
                    <ul class="copyright">
                        <!--<li>Last modified: Dec 27, 2021</li>-->
                    </ul>
                </div>
            </div>
        </footer>
        <!-- Footer End-->
</body>

</html>
